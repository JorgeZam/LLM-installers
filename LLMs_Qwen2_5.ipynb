{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j_cM-P7lrx-"
   },
   "source": [
    "Instalación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install qwen-vl-utils\n",
    "!pip install openai\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU está disponible:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU NO está disponible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxsxgFZPmMrO"
   },
   "source": [
    "Descargar una imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://cdn.resfu.com/media/img_news/zinedine-zidane--en-un-partido-de-champions-league-con-el-real-madrid.jpg?size=1000x&lossy=1\"\n",
    "img = wget.download(url)\n",
    "imagen = Image.open(img)\n",
    "\n",
    "\n",
    "# Show the image\n",
    "from IPython.display import Image\n",
    "Image(filename=img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "checkpoint = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")#, attn_implementation=\"flash_attention_2\") #attn_implementation=\"flash_attention_2\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "prompt = 'Describe the given image', # Ajustar el prompt a las necesidades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "          {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [{\"type\":\"text\",\"text\": \"You are a helpful assistant.\"}]\n",
    "          },\n",
    "\n",
    "          {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(img)}\"},\n",
    "              },\n",
    "              {\"type\": \"text\", \"text\": prompt},\n",
    "          ],\n",
    "          }\n",
    "  ]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "\n",
    "inputs = processor(text=[text], images = [imagen],  padding=True, return_tensors=\"pt\")\n",
    "inputs = inputs.to('cuda')\n",
    "\n",
    "output_ids = model.generate(**inputs, max_new_tokens=4096)\n",
    "generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(output_text[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
